{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPG40CNB05MeVdhAMMXDlEW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VictorMadu/Stackover-question-tagging/blob/master/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn6w8NAqPtpq",
        "colab_type": "code",
        "outputId": "a5fd04e9-7d84-4a3a-bc1e-970f7cf7bdb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QriotCVVXvDS",
        "colab_type": "code",
        "outputId": "5cb989cc-6a0d-4209-e9ba-3b44ee3c8eca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import sqlite3\n",
        "import io\n",
        "import dill\n",
        "from keras.layers import BatchNormalization, Dropout, Dense, Input, concatenate\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT8ZbM63vTlK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 50000\n",
        "vectorizer = dill.load(open('/content/drive/My Drive/tag_vectorizer.p', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6Urc2e3NRQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class TrainGenerator(keras.utils.Sequence):\n",
        "  'Generates data for Keras'\n",
        "  def __init__(self, path, val_indices=[]):\n",
        "    'Initialization'\n",
        "    sqlite3.register_adapter(np.ndarray, self.adapt_array)\n",
        "    sqlite3.register_converter(\"array\", self.convert_array)\n",
        "    self.path = path\n",
        "    self.conn = sqlite3.connect(self.path, detect_types=sqlite3.PARSE_DECLTYPES, check_same_thread=False)\n",
        "    self.cur = self.conn.cursor()\n",
        "    self.shuffle_index = np.arange(self.__len__())\n",
        "    self.shuffle_index = np.delete(self.shuffle_index, val_indices)\n",
        "    np.random.shuffle(self.shuffle_index)\n",
        "    self.number_of_batches = int(np.ceil(self.__len__() // batch_size))\n",
        "    \n",
        "\n",
        "  def __len__(self):\n",
        "    sql_query = \"\"\"SELECT COUNT(*) FROM data\"\"\"\n",
        "    self.cur.execute(sql_query)\n",
        "    value = self.cur.fetchone()\n",
        "    return value[0]\n",
        "\n",
        "\n",
        "  def __getitem__(self, batch_size):\n",
        "    'Gets batches of data'\n",
        "    while True:\n",
        "      self.counter = 0\n",
        "      for i in range(self.number_of_batches):\n",
        "        if self.number_of_batches > self.counter:\n",
        "          index_batch = self.shuffle_index[batch_size*self.counter:batch_size*(self.counter+1)]\n",
        "          self.counter += 1\n",
        "          yield self.__data_generation(index_batch, batch_size)\n",
        "          print('Batch', i+1, \"loaded\")\n",
        "        else: \n",
        "          index_batch = self.shuffle_index[batch_size*self.counter:]\n",
        "          yield self.__data_generation(index_batch, batch_size)\n",
        "          self.counter = 0\n",
        "          print('Batch', i+1, \"loaded\")\n",
        "          \n",
        "\n",
        "\n",
        "\n",
        "  def __data_generation(self, index_batch, batch_size):\n",
        "    'Generates data containing batch_size samples'\n",
        "    sql_query = \"\"\"SELECT title_vec, question_vec, code_vec, len_title, len_question, tags FROM data WHERE Id in {}\"\"\".format(tuple(index_batch))\n",
        "    self.cur.execute(sql_query)\n",
        "    x1 = np.empty((batch_size, 128))\n",
        "    x2 = np.empty((batch_size, 128))\n",
        "    x3 = np.empty((batch_size, 128))\n",
        "    x4 = np.empty((batch_size, 1))\n",
        "    x5 = np.empty((batch_size, 1))\n",
        "    y = np.empty((batch_size, 5500))\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      line = self.cur.fetchone()\n",
        "      x1[i, :] = line[0]\n",
        "      x2[i, :] = line[1]\n",
        "      x3[i, :] = line[2]\n",
        "      x4[i, :] = line[3]\n",
        "      x5[i, :] = line[4]\n",
        "      y[i, :] = vectorizer.transform([line[5]]).toarray()\n",
        "    return ([x1, x2, x3, x4, x5], y)\n",
        "\n",
        "  def adapt_array(arr):\n",
        "    out = io.BytesIO()\n",
        "    np.save(out, arr)\n",
        "    out.seek(0)\n",
        "    return sqlite3.Binary(out.read())\n",
        "\n",
        "  def convert_array(self, text):\n",
        "    out = io.BytesIO(text)\n",
        "    out.seek(0)\n",
        "    return np.load(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exxhPau7X9Io",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ValidationGenerator(keras.utils.Sequence):\n",
        "  def __init__(self, path):\n",
        "    self.path = path\n",
        "    sqlite3.register_adapter(np.ndarray, self.adapt_array)\n",
        "    sqlite3.register_converter(\"array\", self.convert_array)\n",
        "    self.conn = sqlite3.connect(self.path, detect_types=sqlite3.PARSE_DECLTYPES)\n",
        "    self.cur = self.conn.cursor()\n",
        "    self.shuffle_index = np.arange(self.__len__())\n",
        "    np.random.shuffle(self.shuffle_index)\n",
        "\n",
        "\n",
        "  def __getitem__(self, batch_size):\n",
        "    index_batch = self.shuffle_index[:batch_size]\n",
        "    return self.__data_generation(index_batch, batch_size)\n",
        "\n",
        "  def __len__(self):\n",
        "    sql_query = \"\"\"SELECT COUNT(*) FROM data\"\"\"\n",
        "    self.cur.execute(sql_query)\n",
        "    value = self.cur.fetchone()\n",
        "    return value[0]\n",
        "    \n",
        "\n",
        "  def __data_generation(self, index_batch, batch_size):\n",
        "    'Generates data containing batch_size samples'\n",
        "    sql_query = \"\"\"SELECT title_vec, question_vec, code_vec, len_title, len_question,  tags FROM data WHERE Id in {}\"\"\".format(tuple(index_batch))\n",
        "    self.cur.execute(sql_query)\n",
        "    x1 = np.empty((batch_size, 128))\n",
        "    x2 = np.empty((batch_size, 128))\n",
        "    x3 = np.empty((batch_size, 128))\n",
        "    x4 = np.empty((batch_size, 1))\n",
        "    x5 = np.empty((batch_size, 1))\n",
        "    y = np.empty((batch_size, 5500))\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      line = self.cur.fetchone()\n",
        "      x1[i, :] = line[0]\n",
        "      x2[i, :] = line[1]\n",
        "      x3[i, :] = line[2]\n",
        "      x4[i, :] = line[3]\n",
        "      x5[i, :] = line[4]\n",
        "      y[i, :] = vectorizer.transform([line[5]]).toarray()\n",
        "    return ([x1, x2, x3, x4, x5], y, index_batch)\n",
        "\n",
        "\n",
        "  def adapt_array(arr):\n",
        "    out = io.BytesIO()\n",
        "    np.save(out, arr)\n",
        "    out.seek(0)\n",
        "    return sqlite3.Binary(out.read())\n",
        "\n",
        "  def convert_array(self, text):\n",
        "    out = io.BytesIO(text)\n",
        "    out.seek(0)\n",
        "    return np.load(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4YpmU6wiH4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation = ValidationGenerator(\"/content/drive/My Drive/Question.sqlite\")\n",
        "X_val, y_val, val_indices = validation.__getitem__(100000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMeTA5dfUYMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = TrainGenerator(\"/content/drive/My Drive/Question.sqlite\", val_indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LJ5QvlzIy-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E97mkEpRjEJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nn_model():\n",
        "  print('Building model...')\n",
        "  seq1 = Input(shape=(128,))\n",
        "  seq2 = Input(shape=(128,))\n",
        "  seq3 = Input(shape=(128,))\n",
        "  seq4 = Input(shape=(1,))\n",
        "  seq5 = Input(shape=(1,))\n",
        "\n",
        "  dense1 = Dropout(0.2)(seq1)\n",
        "  dense1 = Dense(1024, activation='relu')(dense1)\n",
        "  dense1 = Dense(1024, activation='relu')(dense1)\n",
        "\n",
        "  dense2 = Dropout(0.2)(seq2)\n",
        "  dense2 = Dense(1024, activation='relu')(dense2)\n",
        "  dense2 = Dense(1024, activation='relu')(dense2)\n",
        "\n",
        "  dense3 = Dropout(0.2)(seq3)\n",
        "  dense3 = Dense(1024, activation='relu')(dense3)\n",
        "  dense3 = Dense(1024, activation='relu')(dense3)\n",
        "\n",
        "  merge = concatenate([dense1, dense2, dense3, seq4, seq5])\n",
        "  merge = Dropout(0.2)(merge)\n",
        "  merge = BatchNormalization()(merge)\n",
        "  merge = Dense(3072, activation='relu')(merge)\n",
        "  merge = Dense(3072, activation='relu')(merge)\n",
        "\n",
        "  merge = Dropout(0.2)(merge)\n",
        "  merge = BatchNormalization()(merge)\n",
        "  pred = Dense(5500, activation='softmax')(merge)\n",
        "  \n",
        "  model = Model(inputs=[seq1, seq2, seq3, seq4, seq5], outputs=pred)\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rduY2G_ppAQE",
        "colab_type": "code",
        "outputId": "40159ecc-df78-40ae-d769-a4a329a7a319",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', patience=4)\n",
        "mc = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "\n",
        "model = nn_model()\n",
        "history = model.fit_generator(generator=train.__getitem__(batch_size), \n",
        "                                  steps_per_epoch=2, \n",
        "                                        callbacks = [es, mc], \n",
        "                                           validation_data=(X_val, y_val) \n",
        "                                           )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building model...\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:709: UserWarning: An input could not be retrieved. It could be because a worker has died.We do not have any information on the lost sample.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 1 loaded\n",
            "Batch 2 loaded\n",
            "Batch 3 loaded\n",
            "Batch 4 loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8xD7RTNYvIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}